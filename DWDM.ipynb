{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DWDM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UtMfS-1H6x5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "ae1d3a4c-51ab-4d5b-d991-ef7a5feff6e6"
      },
      "source": [
        "\n",
        "library(twitteR)\n",
        "library(tm)\n",
        "library(wordcloud)\n",
        "library(RColorBrewer)\n",
        "library(topicmodels)\n",
        "library(lda)\n",
        "library(MASS)\n",
        "library(openNLP)\n",
        "library(NLP)\n",
        "library(R.utils)\n",
        "library(msm)\n",
        "library(stringdist)\n",
        "library(igraph)\n",
        "library(stats)\n",
        "library(utils)\n",
        "library(devtools)\n",
        "devtools::install_github(\"hadley/stringr\")\n",
        "\n",
        "f <- \"/Users/shashank/Documents/PGDBA/ISI\\ 1st\\ Sem/Computing\\ for\\ Data\\ Science/Project/comments_edited.csv\"\n",
        "df_tmp <- read.csv(f)\n",
        "\n",
        "\n",
        "h_dictionary=read.csv(\"/Users/shashank/Documents/PGDBA/ISI\\ 1st\\ Sem/Computing\\ for\\ Data\\ Science/Project/HindiDictionary.csv\")\n",
        "h_dictionary=as.character(h_dictionary[,1])\n",
        "\n",
        "chunk <- 5 \n",
        "df_comments <- data.frame(comment=NA)\n",
        "i <- 0\n",
        "\n",
        "\n",
        "corrected_lookup <- matrix(data = c(\"guys\",\"guy\"),ncol = 2)\n",
        "corrected_lookup <- read.table(\"/Users/shashank/Documents/PGDBA/ISI\\ 1st\\ Sem/Computing\\ for\\ Data\\ Science/Project/corrected_lookup.csv\")\n",
        "\n",
        "\n",
        "while (chunk*(i+1) <= nrow(df_tmp)) {\n",
        "  c <- paste(df_tmp[((chunk*i)+1):(chunk*(i+1)),1], collapse = \" \")\n",
        "  c <- gsub(\"[[:punct:]]\", \" \",c)\n",
        "  words <- tolower(unlist(strsplit(c, \"\\\\s+\")))\n",
        "  words <- words[!words %in% h_dictionary]\n",
        "  corrected_words <- c()\n",
        "  for (w in words) {\n",
        "    if (is.na(match(w,sorted_words))) {\n",
        "      if (is.na(match(w,corrected_lookup[,1]))) {\n",
        "        rep <- mycorrect(w)\n",
        "        corrected_words <- c(corrected_words,rep)\n",
        "        if (!(rep==w)) {\n",
        "          corrected_lookup <- rbind(corrected_lookup,cbind(w,rep))\n",
        "        } \n",
        "      } else {\n",
        "        corrected_words <- c(corrected_words,corrected_lookup[match(corrected_lookup[,1],w),1])\n",
        "      }\n",
        "      \n",
        "    } else {\n",
        "      corrected_words <- c(corrected_words,w)\n",
        "    }\n",
        "  }\n",
        "  \n",
        "  df_comments <- rbind(df_comments,paste0(corrected_words, collapse = \" \"))\n",
        "  i <- i+1\n",
        "}\n",
        "df_comments <- as.data.frame(df_comments[-1,])\n",
        "\n",
        "\n",
        "write.table(as.data.frame(corrected_lookup),file = \"/Users/shashank/Documents/PGDBA/ISI\\ 1st\\ Sem/Computing\\ for\\ Data\\ Science/Project/corrected_lookup.csv\",col.names = NA)\n",
        "\n",
        "\n",
        "str(df_comments)\n",
        "content=df_comments[,1]\n",
        "class(content)\n",
        "x <- matrix(data=\"\",nrow = length(content), ncol = 2)\n",
        "colnames(x)=c(\"Content\",\"Nouns+verbs+Adjectives\")\n",
        "\n",
        "Maxent_Sent_Token_Annotator(language = \"en\", probs = FALSE, model = NULL)\n",
        "sent_token_annotator <- Maxent_Sent_Token_Annotator()\n",
        "word_token_annotator <- Maxent_Word_Token_Annotator()\n",
        "pos_tag_annotator <- Maxent_POS_Tag_Annotator()\n",
        "class(content)\n",
        "for(j in 1:length(content)) {\n",
        "  trim <- function (a) gsub(\"^\\\\s+|\\\\s+$\", \"\", a)\n",
        "  s=trim(content[j])  \n",
        "  if(nchar(s)==0 || is.na(s)) {\n",
        "    next \n",
        "  }\n",
        "  s=as.character(s)\n",
        "  s <- paste(c(s), collapse = \"\")\n",
        "  s <- as.String(s)\n",
        "  a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))\n",
        "  a3 <- annotate(s, pos_tag_annotator, a2)\n",
        "  a3w <- subset(a3, type == \"word\")\n",
        "  tags <- sapply(a3w$features, `[[`, \"POS\")\n",
        "  class(a3w)\n",
        "  class(a3w$features)\n",
        "  a3wdf=as.data.frame(a3w)\n",
        "  a4w=cbind(a3wdf,tags)\n",
        "  i=1\n",
        "  x[j,1]=as.character(data[j,1])\n",
        "  for(i in 1:nrow(a4w)) {\n",
        "    if(a4w[i,6]==\"NN\"|a4w[i,6]==\"NNS\"|a4w[i,6]==\"NNP\"|a4w[i,6]==\"NNPS\"){\n",
        "      x[j,2]=paste((x[j,2]),(s[a4w[i,3],a4w[i,4]]),sep=\" \")\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "# creating a corpus from the pre-processed data frame and further post-processing\n",
        "mach_corpus = Corpus(VectorSource(x[,2]))\n",
        "myCorpus <- tm_map(mach_corpus,content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),\n",
        "                   mc.cores=1)\n",
        "\n",
        "removeURL <- function(x) gsub(\"http[[:alnum:]]*\", \"\", x)\n",
        "myCorpus <- tm_map(myCorpus, removeURL)\n",
        "myCorpus <- tm_map(myCorpus, tolower)\n",
        "myCorpus <- tm_map(myCorpus, removePunctuation)\n",
        "myCorpus <- tm_map(myCorpus, removeNumbers)\n",
        "myCorpus <- tm_map(myCorpus, PlainTextDocument)\n",
        "\n",
        "# creating document term matrix\n",
        "myDtm <- DocumentTermMatrix(myCorpus, control = list(wordLengths=c(2,Inf), weighting=weightTf))\n",
        "#myDtm_smart <- DocumentTermMatrix(myCorpus, control = list(wordLengths=c(2,Inf), weighting= function(x) weightSMART(x,spec = \"Lnn\")))\n",
        "\n",
        "# tfidf weighing in dtm\n",
        "myDtm_tfidf <- DocumentTermMatrix(myCorpus, control = list(wordLengths=c(2,Inf), weighting=weightTfIdf))\n",
        "myDtm$i <- myDtm_tfidf$i\n",
        "myDtm$j <- myDtm_tfidf$j\n",
        "myDtm$v <- as.integer(trunc((myDtm_tfidf$v)*10^(ceiling(log10(1/min((myDtm_tfidf$v)))))))\n",
        "\n",
        "########################################################################################\n",
        "\n",
        "\n",
        "######################## lda and epistemic mapping of topics ##########################\n",
        "\n",
        "dtm <- myDtm\n",
        "rowTotals <- apply(dtm, 1, sum)\n",
        "dtm2 <- dtm[rowTotals>0,]\n",
        "dim(dtm2)\n",
        "\n",
        "# running lda with 10 topics\n",
        "dtm_LDA <- LDA(dtm2, 10)\n",
        "# getting 10 most relevant keywords for each topic, just for a quick look\n",
        "get_terms(dtm_LDA, 10) \n",
        "\n",
        "# dictionary-topic mapping based on similarity of relevant words in the reference topics\n",
        "# and topics found by applying lda over the corpus of comments\n",
        "\n",
        "# loading the distribution of words for each reference topic already created\n",
        "topics_path <- \"/Users/shashank/Documents/PGDBA/ISI\\ 1st\\ Sem/Computing\\ for\\ Data\\ Science/Project/Topics/\"\n",
        "filenames <- list.files(topics_path,pattern=\"*.csv\")\n",
        "f_path <- paste0(topics_path,filenames,sep=\"\")\n",
        "dic <- list()\n",
        "for (i in 1:length(filenames)) {\n",
        "  dic[[i]] <- as.matrix(read.csv(f_path[i],sep = \",\"))\n",
        "  cum_prob <- 0\n",
        "  crt_len <- 0\n",
        "  for (j in 1:nrow(dic[[i]])) {\n",
        "    cum_prob = cum_prob + as.numeric(dic[[i]][j,3])\n",
        "    if (cum_prob>=0.95) {\n",
        "      crt_len <- j\n",
        "      break\n",
        "    }\n",
        "  }\n",
        "  if (crt_len!=0) {\n",
        "    dic[[i]] <- as.matrix(read.csv(f_path[i],sep = \",\"))[1:crt_len,2:3]\n",
        "  } else {\n",
        "    dic[[i]] <- as.matrix(read.csv(f_path[i],sep = \",\"))[,2:3]\n",
        "  }\n",
        "  dic[[i]][,2] <- matrix(as.numeric(dic[[i]][,2])/sum(as.numeric(dic[[i]][,2])),ncol = 1)\n",
        "  \n",
        "}\n",
        "\n",
        "dic_len=nrow(dic[[1]])\n",
        "lda_topic_words <- list()\n",
        "dtm_LDA_beta <- exp(dtm_LDA@beta)\n",
        "for (i in 1:nrow(dtm_LDA@beta)) {\n",
        "  tmp_beta <- sort(dtm_LDA_beta[i,],decreasing = TRUE)\n",
        "  cum_prob <- 0\n",
        "  crt_len <- 0\n",
        "  \n",
        "  for (j in 1:dic_len) {\n",
        "    cum_prob <- cum_prob+tmp_beta[j]\n",
        "    if (cum_prob>0.95) {\n",
        "      crt_len <- j\n",
        "      break\n",
        "    }\n",
        "  }\n",
        "  if (crt_len!=0) {\n",
        "    lda_topic_words[[i]] <- t(as.matrix(rbind(get_terms(dtm_LDA,crt_len),tmp_beta[1:crt_len])))\n",
        "  } else {\n",
        "    lda_topic_words[[i]] <- as.matrix(cbind(as.vector(get_terms(dtm_LDA,dic_len)),matrix(tmp_beta[1:dic_len],ncol = 1)))\n",
        "  }\n",
        "  lda_topic_words[[i]][,2] <- matrix(as.numeric(lda_topic_words[[i]][,2])/sum(as.numeric(lda_topic_words[[i]][,2])),ncol = 1)\n",
        "}\n",
        "\n",
        "# calculating the similarity of topics in the corpus of comments with every dictionary \n",
        "# topic based on the dot product of probability values for same words in the topics\n",
        "smlr <- matrix(data = NA,ncol = length(dic),nrow = dtm_LDA@k)\n",
        "topic_dic_index <- matrix(data = NA,ncol = 1,nrow = dtm_LDA@k)\n",
        "for (i in 1:dtm_LDA@k) {\n",
        "  for (j in 1:length(dic)) {\n",
        "    common_lexicon <- unique(c(lda_topic_words[[i]][,1],dic[[j]][,1]))\n",
        "    s <- 0\n",
        "    for (w in common_lexicon) {\n",
        "      if (!(is.na(match(w,lda_topic_words[[i]][,1]))) & !(is.na(match(w,dic[[j]][,1])))) {\n",
        "        s <- s + as.numeric(lda_topic_words[[i]][(match(w,lda_topic_words[[i]][,1])),2])*as.numeric(dic[[j]][(match(w,dic[[j]][,1])),2])\n",
        "      }\n",
        "    }\n",
        "    smlr[i,j] <- s\n",
        "  }\n",
        "  topic_dic_index[i,1] <- gsub(\"+\\\\s.csv\",\"\",filenames[which.max(smlr[i,])])\n",
        "}\n",
        "\n",
        "#######################################################################################\n",
        "\n",
        "\n",
        "#################### msm based markov chain modeling ##################################\n",
        "\n",
        "# creating the batch of data required by msm for markov chain modeling\n",
        "n_indv <- 1\n",
        "time_factor <- 1\n",
        "epoch <- length(dtm_LDA@documents)\n",
        "time_frame <- as.integer(trunc(epoch*time_factor))\n",
        "topic_set <- seq(1,dtm_LDA@k,1)\n",
        "df_msm <- data.frame(Ints=integer(),\n",
        "                     Ints=integer(),\n",
        "                     Ints=integer())\n",
        "\n",
        "for (i in 1:n_indv) {\n",
        "  df_time <- t(t(sort(sample(seq(1,epoch,1),size = time_frame,replace = FALSE))))\n",
        "  df_topic <- c()\n",
        "  for (j in df_time) {\n",
        "    t_prob <- dtm_LDA@gamma[j,]\n",
        "    #   df_topic <- c(df_topic,sample(topic_set,size = 1,prob = t_prob))\n",
        "    df_topic <- c(df_topic,which.max(t_prob))\n",
        "  }\n",
        "  df_topic <- t(t(df_topic))\n",
        "  df_tmp <- cbind((vector(length = time_frame)+i),df_time,df_topic)\n",
        "  df_msm <- rbind(df_msm,df_tmp)\n",
        "}\n",
        "\n",
        "colnames(df_msm) <- c(\"id\",\"time\",\"state\")\n",
        "\n",
        "\n",
        "# creating initial qmatrix and transition probability matrix as required by msm\n",
        "qmatrix <- mat.or.vec(length(topic_set),length(topic_set))+1\n",
        "init_tp <- statetable.msm(state, id, data=df_msm)\n",
        "init_tp <- init_tp/rowSums(init_tp) \n",
        "crudeinits.msm(state ~ time, id, data=df_msm, qmatrix=qmatrix)\n",
        "\n",
        "# running msm for fitting DTMC using 'bfgs' optimizer and other parameters to esure\n",
        "# convergence while mainting accuracy of the maximum likihood estimates for the \n",
        "# transition probability parameters\n",
        "\n",
        "topic.msm <- msm( formula = state ~ time, subject = id, data = df_msm,qmatrix = qmatrix,gen.inits = TRUE,\n",
        "                  obstype = 2,exacttimes = TRUE,control = list(fnscale = 2500,\n",
        "                                                               reltol = 1e-16,ndeps=rep(1e-6, 90)))\n",
        "\n",
        "\n",
        "# splitting the chain in lengths of 100 time units to study the time in-homogeneous\n",
        "# nature of the markov chain under time-series model\n",
        "pmatrix_list=list()\n",
        "topic.msm_list=list()\n",
        "pointer=1\n",
        "i=1\n",
        "for (j in seq(100,nrow(df_msm),100)) {\n",
        "  topic.msm <- msm( formula = state ~ time, subject = id, data = df_msm[pointer:j,],qmatrix = qmatrix,gen.inits = TRUE,\n",
        "                    obstype = 2,exacttimes = TRUE,method = \"CG\")\n",
        "  topic.msm_list[[i]] = topic.msm\n",
        "  pmatrix_list[[i]] = as.matrix(pmatrix.msm(topic.msm))\n",
        "  pointer=j+1\n",
        "  i=i+1\n",
        "}\n",
        "\n",
        "ptrans_mat =matrix(data = 0,nrow = length(pmatrix_list),ncol = length(pmatrix_list[[1]]) )\n",
        "for (i in (1:length(pmatrix_list))) {\n",
        "  ptrans_mat[i,] = c(pmatrix_list[[i]])\n",
        "}\n",
        "\n",
        "# plotting the distribution of a few transition probability numbers over time\n",
        "qplot(seq(1,64,1),ptrans_mat[,1],main=\"Distribution of p(1,1) over time\", xlab=\"Time Window\",ylab=\"p(1,1)\")\n",
        "qplot(seq(1,64,1),ptrans_mat[,10],main=\"Distribution of p(10,1) over time\", xlab=\"Time Window\",ylab=\"p(10,1)\")\n",
        "qplot(seq(1,64,1),ptrans_mat[,91],main=\"Distribution of p(1,10) over time\", xlab=\"Time Window\",ylab=\"p(1,10)\")\n",
        "qplot(seq(1,64,1),ptrans_mat[,100],main=\"Distribution of p(10,10) over time\", xlab=\"Time Window\",ylab=\"p(10,10)\")\n",
        "\n",
        "#######################################################################################\n",
        "\n",
        "\n",
        "################## visualization in igraph ######################################################\n",
        "\n",
        "test_nodes <- read.csv(\"G:\\\\PGDBA\\\\ISI\\\\Computing for Data Science\\\\CDS Project\\\\graphs\\\\nodes.csv\", header=F, as.is=T)\n",
        "test_links <- read.csv(\"G:\\\\PGDBA\\\\ISI\\\\Computing for Data Science\\\\CDS Project\\\\graphs\\\\links.csv\", header=F, as.is=T)\n",
        "transition <- read.csv(\"G:\\\\PGDBA\\\\ISI\\\\Computing for Data Science\\\\CDS Project\\\\graphs\\\\transition_matrix.csv\", header=T, as.is=T)\n",
        "transition <- transition[,2:length(transition)]\n",
        "trans_mat <- as.matrix(transition)\n",
        "\n",
        "trans=c()\n",
        "for(i in 1:length(transition)){\n",
        "  trans = append(trans,as.matrix(transition)[i,])\n",
        "}\n",
        "\n",
        "x_coor=c(2,2,4,4)\n",
        "y_coor=c(1,4,1,4)\n",
        "l=as.matrix(cbind(x_coor,y_coor))\n",
        "\n",
        "state1=t(as.matrix(test_nodes[,2]))\n",
        "step=10\n",
        "docs=4\n",
        "png(file=\"G:\\\\PGDBA\\\\ISI\\\\Computing for Data Science\\\\CDS Project\\\\graphs\\\\example%03d.png\", width=1600,height=900)\n",
        "\n",
        "for(i in 1:docs){  \n",
        "  state2=state1%*%trans_mat\n",
        "  for(j in 0:step){  \n",
        "    temp_nodes=state1+(state2-state1)*j/step\n",
        "    temp_nodes=t(temp_nodes)\n",
        "    temp_nodes=as.data.frame(cbind(c(\"\",\"T2\",\"T3\",\"T4\"),temp_nodes))\n",
        "    rownames(temp_nodes)=NULL\n",
        "    temp_nodes$V2=as.numeric(levels(temp_nodes$V2))[temp_nodes$V2]\n",
        "    test_net <- graph.data.frame(test_links, temp_nodes, directed=T)\n",
        "    #test_net\n",
        "    #V(test_net)$name\n",
        "    #V(test_net)$V2\n",
        "    #to remove links to self\n",
        "    #plot(test_net)\n",
        "    #plot(test_net, edge.arrow.size=.4)\n",
        "    \n",
        "    \n",
        "    #colrs <- c(\"Red\", \"Green\", \"Blue\", \"Yellow\")\n",
        "    \n",
        "    color=c(rgb((1-round(100*V(test_net)$V2[1])/100),(1-round(100*V(test_net)$V2[1])/100),(round(100*V(test_net)$V2[1])/100)),\n",
        "            rgb((1-round(100*V(test_net)$V2[2])/100),(1-round(100*V(test_net)$V2[2])/100),(round(100*V(test_net)$V2[2])/100)),\n",
        "            rgb((1-round(100*V(test_net)$V2[3])/100),(1-round(100*V(test_net)$V2[3])/100),(round(100*V(test_net)$V2[3])/100)),\n",
        "            rgb((1-round(100*V(test_net)$V2[4])/100),(1-round(100*V(test_net)$V2[4])/100),(round(100*V(test_net)$V2[4])/100)))\n",
        "    \n",
        "    \n",
        "    # plot(test_net,vertex.size=200*V(test_net)$V2,edge.arrow.size=.6,layout=l,vertex.color=color,\n",
        "    #     vertex.label=as.character(V(test_net)$name),main=paste(\"time=\",as.character(i+j/step)))\n",
        "    \n",
        "    test_net <- simplify(test_net, remove.multiple = F, remove.loops = T)\n",
        "    \n",
        "    plot(test_net,vertex.size=200*V(test_net)$V2,vertex.label=as.character(V(test_net)$name),\n",
        "         edge.arrow.size=.6,layout=l,vertex.color=color,main=paste(\"time=\",as.character(i+j/step)))\n",
        "  }\n",
        "  state1=state2\n",
        "  rm(temp_nodes)\n",
        "}\n",
        "\n",
        "########################################################################################\n",
        "\n",
        "\n",
        "####################### native modules #################################################\n",
        "\n",
        "# java based crawler for web scraping\n",
        "\n",
        "package myGov;\n",
        "\n",
        "import java.io.*;\n",
        "import java.io.BufferedReader;\n",
        "import java.io.File;\n",
        "import java.io.FileNotFoundException;\n",
        "import java.io.FileReader;\n",
        "import java.io.IOException;\n",
        "import java.io.InputStream;\n",
        "import java.io.InputStreamReader;\n",
        "import java.net.HttpURLConnection;\n",
        "import java.net.URL;\n",
        "import java.net.URLConnection;\n",
        "import java.util.ArrayList;\n",
        "import java.util.Date;\n",
        "import java.util.Iterator;\n",
        "import java.util.List;\n",
        "\n",
        "import javax.net.ssl.HttpsURLConnection;\n",
        "import javax.swing.text.Document;\n",
        "import javax.swing.text.EditorKit;\n",
        "import javax.swing.text.SimpleAttributeSet;\n",
        "import javax.swing.text.html.HTML;\n",
        "import javax.swing.text.html.HTMLDocument;\n",
        "import javax.swing.text.html.HTMLEditorKit;\n",
        "\n",
        "import jxl.Workbook;\n",
        "import jxl.write.Label;\n",
        "import jxl.write.WritableSheet;\n",
        "import jxl.write.WritableWorkbook;\n",
        "\n",
        "import org.jsoup.Jsoup;\n",
        "import org.jsoup.select.Elements;\n",
        "\n",
        "public class Scrap {\n",
        "  public static void main(String[] args) throws Exception, IOException {\n",
        "    int file_count = 83;\n",
        "    BufferedWriter bw1 = new BufferedWriter(new FileWriter(\n",
        "      \"G:/EclipseWorkspace/scrapping/2_Bhopal_output.csv\"));\n",
        "    int v = 4383;\n",
        "    int count = 0;\n",
        "    BufferedWriter bw = new BufferedWriter(new FileWriter(\n",
        "      \"G:/EclipseWorkspace/scrapping/Bhopal_output_\" + file_count\n",
        "      + \".csv\"));\n",
        "    do {\n",
        "      // define baseurl here\n",
        "      String govbase = \"https://mygov.in/group-issue/smart-city-bhopal/?field_hashtags_tid=&sort_by=created&sort_order=DESC&page=0%2C\";\n",
        "      String govlink = govbase + v;\n",
        "      System.out.println(v);\n",
        "      // / the below code will take all links from discussions page\n",
        "      \n",
        "      String govres = getUrlSource(govlink);\n",
        "      // System.out.println(govres);\n",
        "      org.jsoup.nodes.Document doc = Jsoup.parse(govres, \"UTF-8\");\n",
        "      Elements discussionlink = doc.select(\"div.comment_body\");\n",
        "      // String f1 = discussionlink.toString();\n",
        "      // System.out.println(f1);\n",
        "      Elements c1 = discussionlink.select(\"p\");\n",
        "      String s1 = c1.toString();\n",
        "      String comments[] = s1.split(\"</p>\\n<p>\");\n",
        "      \n",
        "      for (int i = 0; i < comments.length; i++) {\n",
        "        comments[i] = comments[i].replaceAll(\"\\\\<[^>]*>\", \"\");\n",
        "        comments[i] = comments[i].replace(\",\", \"\");\n",
        "        bw.write(comments[i] + \" \");\n",
        "        bw1.write(comments[i]);\n",
        "        bw1.newLine();\n",
        "        count++;\n",
        "        if (count == 500) {\n",
        "          Elements time = doc.select(\"span.date_time\");\n",
        "          String f2 = time.toString();\n",
        "          String s2 = f2.replace(\"<span class=\\\"date_time\\\">\", \"\")\n",
        "          .replace(\"</span>\", \"\");\n",
        "          String times[] = s2.split(\"\\n\");\n",
        "          bw.write(\",\" + times[0]);\n",
        "          bw.close();\n",
        "          file_count++;\n",
        "          count = 0;\n",
        "          bw = new BufferedWriter(new FileWriter(\n",
        "            \"G:/EclipseWorkspace/scrapping/Bhopal_output_\"\n",
        "            + file_count + \".csv\"));\n",
        "        }\n",
        "      }\n",
        "      v++;\n",
        "    } while (v <= 5000);\n",
        "    bw1.close();\n",
        "  }\n",
        "  \n",
        "  private static String getUrlSource(String govlink) throws IOException {\n",
        "    // System.out.println(\"inside geturlsource\");\n",
        "    URL userurl = new URL(govlink);\n",
        "    HttpsURLConnection input= (HttpsURLConnection) userurl\n",
        "    .openConnection();\n",
        "    BufferedReader in = new BufferedReader(new InputStreamReader(\n",
        "      input.getInputStream(), \"UTF-8\"));\n",
        "    String inputLine;\n",
        "    StringBuilder a = new StringBuilder();\n",
        "    while ((inputLine = in.readLine()) != null)\n",
        "      a.append(inputLine);\n",
        "    in.close();\n",
        "    \n",
        "    return a.toString();\n",
        "  }\n",
        "  \n",
        "}\n",
        "\n",
        "# aspell based spell_checker\n",
        "\n",
        "raw_data=read.csv(\"G:\\\\EclipseWorkspace\\\\scrapping\\\\comments.csv\",header=FALSE)\n",
        "nrow(raw_data)\n",
        "doc=500\n",
        "comment=65\n",
        "data=data.frame(Comment=NA)\n",
        "for(i in 1:doc){\n",
        "  print(i)\n",
        "  raw_data2=paste(raw_data[(comment*(i-1)+1):(comment*i),1],collapse=\" \")\n",
        "  words <- unlist(strsplit(raw_data2, \"\\\\s+\"))\n",
        "  words <- words[!words %in% dictionary]\n",
        "  for(c in 1:length(words)){\n",
        "    asp=aspell(as.factor(words[c]))\n",
        "    if(length(asp$Suggestion)==1){\n",
        "      if(class(unlist(asp$Suggestion))!=\"NULL\"){\n",
        "        words[c]=unlist(asp$Suggestion)[1]\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  raw_data3=paste(words,collapse=\" \")\n",
        "  data=rbind(data,raw_data3) \n",
        "}\n",
        "\n",
        "\n",
        "# spell_checker based on soundex encoding and full Damerau-Levenshtein distance\n",
        "\n",
        "# read in big.txt, a 6.5 mb collection of different english texts.\n",
        "raw_text <- paste(readLines(\"/Users/shashank/Documents/PGDBA/ISI\\ 1st\\ Sem/Computing\\ for\\ Data\\ Science/Project/big.txt\"), collapse = \" \")\n",
        "# make the text lowercase and split it up creating a huge vector of word tokens.\n",
        "split_text <- strsplit(tolower(raw_text), \"[^a-z]+\")\n",
        "# count the number of different type of words.\n",
        "word_count <- table(split_text)\n",
        "# sort the words and create an ordered vector with the most common type of words first.\n",
        "sorted_words <- names(sort(word_count, decreasing = TRUE))\n",
        "\n",
        "mycorrect <- function(word,dist=4) {\n",
        "  # Calculate the edit distance between the word and all other words in sorted_words.\n",
        "  soundex_dist <- stringdist(word, sorted_words,method = \"soundex\")\n",
        "  soundex_filtered_words =sorted_words[which(soundex_dist==0)]\n",
        "  dl_dist <- stringdist(word, soundex_filtered_words,method = \"dl\")\n",
        "  # Calculate the minimum edit distance to find a word that exists in big.txt \n",
        "  # with a limit of two edits.\n",
        "  min_edit_dist <- min(dl_dist, dist)\n",
        "  # Generate a vector with all words with this minimum edit distance.\n",
        "  # Since sorted_words is ordered from most common to least common, the resulting\n",
        "  # vector will have the most common / probable match first.\n",
        "  proposals_by_prob <- c(soundex_filtered_words[ dl_dist <= min_edit_dist])\n",
        "  # In case proposals_by_prob would be empty we append the word to be corrected...\n",
        "  proposals_by_prob <- c(proposals_by_prob, word)\n",
        "  # ... and return the first / most probable word in the vector.\n",
        "  proposals_by_prob[1]\n",
        "}\n",
        "\n",
        "\n",
        "################################## End ################################################"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-271cf7b30489>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    devtools::install_github(\"hadley/stringr\")\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}